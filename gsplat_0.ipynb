{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18bc3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install tqdm\n",
    "%pip install matplotlib\n",
    "%pip install piq\n",
    "%pip install imageio\n",
    "%pip install opencv-python\n",
    "%pip install tensorboard\n",
    "%pip install pycolmap\n",
    "%pip install pyquaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67807c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "import json\n",
    "import pycolmap\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from pyquaternion import Quaternion  # pip install pyquaternion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a410932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the helper\n",
    "def to_homogeneous(pose_3x4):\n",
    "    bottom_row = torch.tensor([[0, 0, 0, 1]], dtype=pose_3x4.dtype, device=pose_3x4.device)\n",
    "    pose_4x4 = torch.cat([pose_3x4, bottom_row], dim=0)\n",
    "    return pose_4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc18b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colmap_to_nerf_point(points):\n",
    "    # Same flip to move world into NeRF-style frame where Z-forward = negative\n",
    "    return points @ torch.diag(torch.tensor([1.0, -1.0, -1.0], dtype=points.dtype, device=points.device)).T\n",
    "\n",
    "def colmap_to_nerf_pose(pose):\n",
    "    flip = torch.diag(torch.tensor([1.0, -1.0, -1.0], dtype=pose.dtype, device=pose.device))\n",
    "    R = pose[:3, :3] @ flip\n",
    "    t = pose[:3, 3]\n",
    "    return torch.cat([\n",
    "        torch.cat([R, t.view(3, 1)], dim=1),\n",
    "        torch.tensor([[0.0, 0.0, 0.0, 1.0]], dtype=pose.dtype, device=pose.device)\n",
    "    ], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33def09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfm_extract(image_dir, device='cuda'):\n",
    "    \"\"\"\n",
    "    Run SfM pipeline using pycolmap Python API, extract camera poses, intrinsics, and 3D points.\n",
    "    \"\"\"\n",
    "    image_dir = Path(image_dir)\n",
    "    database_path = Path(\"temp/database.db\")\n",
    "    sfm_path = Path(\"temp/sfm_output\")\n",
    "\n",
    "    if sfm_path.exists() and any(sfm_path.iterdir()):\n",
    "        print(f\"[INFO] Loading existing SfM reconstruction from {sfm_path}\")\n",
    "        reconstruction = pycolmap.Reconstruction(str(sfm_path / \"0\"))\n",
    "    else:\n",
    "        # Clean up previous runs\n",
    "        if database_path.exists():\n",
    "            database_path.unlink()\n",
    "        if sfm_path.exists():\n",
    "            shutil.rmtree(sfm_path)\n",
    "        sfm_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 1. Extract features\n",
    "        print(\"[INFO] Extracting features...\")\n",
    "\n",
    "\n",
    "        pycolmap.extract_features(\n",
    "            database_path=str(database_path),\n",
    "            image_path=str(image_dir),\n",
    "            camera_model='PINHOLE',\n",
    "            camera_mode='SINGLE'\n",
    "        )\n",
    "\n",
    "        # 2. Match features\n",
    "        print(\"[INFO] Matching features...\")\n",
    "        pycolmap.match_exhaustive(str(database_path))\n",
    "\n",
    "        # 3. Incremental mapping\n",
    "        print(\"[INFO] Performing incremental mapping...\")\n",
    "        reconstructions = pycolmap.incremental_mapping(\n",
    "            str(database_path),\n",
    "            str(image_dir),\n",
    "            str(sfm_path),\n",
    "            initial_image_pair_callback=lambda: print(\"[INFO] Initial image pair registered.\"),\n",
    "            next_image_callback=lambda: print(\"[INFO] Next image registered.\")\n",
    "        )\n",
    "\n",
    "        if not reconstructions:\n",
    "            raise RuntimeError(\"No reconstructions found\")\n",
    "        reconstruction = reconstructions[0]\n",
    "\n",
    "    print(f\"[INFO] Number of registered images: {len(reconstruction.images)}\")\n",
    "    print(f\"[INFO] Number of 3D points: {len(reconstruction.points3D)}\")\n",
    "\n",
    "    # Extract camera poses and intrinsics\n",
    "    pose_c2w_dict = {}\n",
    "    intrinsics_dict = {}\n",
    "    camera_models = set()\n",
    "\n",
    "    for img_id, img in reconstruction.images.items():\n",
    "        img_name = os.path.basename(img.name)\n",
    "        pose = torch.tensor(img.cam_from_world.matrix(), dtype=torch.float32, device=device)\n",
    "        intrinsics = torch.tensor(img.camera.params, dtype=torch.float32, device=device)\n",
    "\n",
    "        pose_c2w_dict[img_name] = colmap_to_nerf_pose(pose)\n",
    "        intrinsics_dict[img_name] = intrinsics\n",
    "\n",
    "        camera = img.camera\n",
    "        camera_models.add(camera.model)\n",
    "\n",
    "        print(f\"[CAMERA INFO] Image ID: {img_id}\")\n",
    "        print(f\" - Image name: {img.name}\")\n",
    "        print(f\" - Camera ID: {camera.camera_id}\")\n",
    "        print(f\" - Camera model: {camera.model}\")\n",
    "        print(f\" - Image size: {camera.width} x {camera.height}\")\n",
    "        print(f\" - Intrinsic parameters ({len(camera.params)}): {camera.params}\")\n",
    "        print(f\" - cam_from_world:\\n{img.cam_from_world.matrix()}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    if len(camera_models) == 1:\n",
    "        print(f\"[INFO] Single camera model detected: {list(camera_models)[0]}\")\n",
    "    else:\n",
    "        print(f\"[WARNING] Multiple camera models detected: {camera_models}\")\n",
    "\n",
    "    # Extract 3D points\n",
    "    points3D = (\n",
    "        torch.stack([\n",
    "            colmap_to_nerf_point(torch.tensor(p.xyz, dtype=torch.float32, device=device))\n",
    "            for p in reconstruction.points3D.values()\n",
    "        ])\n",
    "        if reconstruction.points3D else\n",
    "        torch.empty((0, 3), device=device)\n",
    "    )\n",
    "\n",
    "    return pose_c2w_dict, intrinsics_dict, points3D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef93302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(points3D: torch.Tensor, N: int, jitter=1e-3):\n",
    "    \"\"\"\n",
    "    Adjust the number of points in points3D to exactly N.\n",
    "    If points3D has more than N points, randomly subsample.\n",
    "    If points3D has fewer than N points, randomly duplicate points with small jitter.\n",
    "\n",
    "    Args:\n",
    "        points3D: (M, 3) tensor of input points\n",
    "        N: desired number of points\n",
    "        jitter: standard deviation of Gaussian noise added to duplicated points\n",
    "\n",
    "    Returns:\n",
    "        (N, 3) tensor of points\n",
    "    \"\"\"\n",
    "    M = points3D.shape[0]\n",
    "    device = points3D.device\n",
    "\n",
    "    if M == N:\n",
    "        return points3D.clone()\n",
    "\n",
    "    elif M > N:\n",
    "        # Subsample without replacement\n",
    "        indices = torch.randperm(M, device=device)[:N]\n",
    "        return points3D[indices].clone()\n",
    "\n",
    "    else:\n",
    "        # Need to upsample\n",
    "        repeats = N // M\n",
    "        remainder = N % M\n",
    "\n",
    "        # Repeat entire set multiple times\n",
    "        points_repeated = points3D.repeat(repeats, 1)\n",
    "\n",
    "        # Sample some more points for remainder\n",
    "        extra_indices = torch.randperm(M, device=device)[:remainder]\n",
    "        extra_points = points3D[extra_indices]\n",
    "\n",
    "        upsampled = torch.cat([points_repeated, extra_points], dim=0)\n",
    "\n",
    "        # Add jitter noise to duplicated points only (not strictly necessary but helps break duplicates)\n",
    "        noise = torch.randn_like(upsampled) * jitter\n",
    "        upsampled += noise\n",
    "\n",
    "        return upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRFDataset(Dataset):\n",
    "    def __init__(self, json_path, image_size=512, device='cuda', sfm_poses=None, sfm_intrinsics=None):\n",
    "        with open(json_path, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "\n",
    "        self.frames = meta['frames']\n",
    "        self.camera_angle_x = meta['camera_angle_x']\n",
    "        self.image_size = image_size\n",
    "        self.device = device\n",
    "        self.base_dir = os.path.dirname(json_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((image_size, image_size))\n",
    "        ])\n",
    "\n",
    "        self.sfm_poses = sfm_poses\n",
    "        self.sfm_intrinsics = sfm_intrinsics\n",
    "\n",
    "    def rescale_intrinsics(self, intrinsics, orig_size, new_size):\n",
    "        scale_x = new_size[0] / orig_size[0]\n",
    "        scale_y = new_size[1] / orig_size[1]\n",
    "\n",
    "        intrinsics = intrinsics.clone()\n",
    "        intrinsics[0] *= scale_x  # fx\n",
    "        intrinsics[1] *= scale_y  # fy\n",
    "        intrinsics[2] *= scale_x  # cx\n",
    "        intrinsics[3] *= scale_y  # cy\n",
    "        return intrinsics\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        img_filename = os.path.basename(frame['file_path']) + '.png'  # or '.jpg'\n",
    "\n",
    "        # Load raw image to get original size\n",
    "        img_path = os.path.join(self.base_dir, frame['file_path'] + '.png')\n",
    "        raw = iio.imread(img_path).astype(np.float32) / 255.0\n",
    "        if raw.shape[-1] == 4:\n",
    "            raw = raw[:, :, :3]\n",
    "\n",
    "        orig_height, orig_width = raw.shape[:2]\n",
    "\n",
    "        # Resize image\n",
    "        image = self.transform(raw).to(self.device)\n",
    "\n",
    "        # Load pose\n",
    "        if self.sfm_poses is not None and img_filename in self.sfm_poses:\n",
    "            pose = to_homogeneous(self.sfm_poses[img_filename]).to(self.device)\n",
    "        else:\n",
    "            # We only need to invert if using NeRF data\n",
    "            pose = torch.inverse(torch.tensor(frame['transform_matrix'], dtype=torch.float32).to(self.device))\n",
    "\n",
    "        # Load and rescale intrinsics\n",
    "        if self.sfm_intrinsics is not None and img_filename in self.sfm_intrinsics:\n",
    "            params = self.sfm_intrinsics[img_filename].cpu()\n",
    "            fx, fy, cx, cy = params[:4]\n",
    "\n",
    "            intrinsics = torch.tensor([fx, fy, cx, cy], dtype=torch.float32)\n",
    "            intrinsics = self.rescale_intrinsics(intrinsics, (orig_width, orig_height), (self.image_size, self.image_size))\n",
    "\n",
    "            K = torch.tensor([\n",
    "                [intrinsics[0], 0, intrinsics[2]],\n",
    "                [0, intrinsics[1], intrinsics[3]],\n",
    "                [0, 0, 1]\n",
    "            ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        else:\n",
    "            # Fallback generic intrinsics from NeRF JSON\n",
    "            focal = 0.5 * self.image_size / np.tan(0.5 * self.camera_angle_x)\n",
    "            K = torch.tensor([\n",
    "                [focal, 0, self.image_size / 2],\n",
    "                [0, focal, self.image_size / 2],\n",
    "                [0, 0, 1]\n",
    "            ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        #print(f\"IMAGE: {img_filename} - POSE: {pose.cpu()}\")\n",
    "        return image, pose, K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_points(points, pose, K):\n",
    "    N = points.shape[0]\n",
    "    device = points.device\n",
    "    points_h = torch.cat([points, torch.ones(N, 1, device=device)], dim=-1)  # (N,4)\n",
    "    cam_points = (pose @ points_h.T).T[:, :3]  # (N,3)\n",
    "\n",
    "    z = cam_points[:, 2]\n",
    "\n",
    "    fx, fy = K[0, 0], K[1, 1]\n",
    "    cx, cy = K[0, 2], K[1, 2]\n",
    "\n",
    "    # Avoid division by zero by clamping z (only for valid points, will handle negative z separately)\n",
    "    z_safe = z.clamp(min=1e-5)\n",
    "\n",
    "    u = fx * cam_points[:, 0] / z_safe + cx\n",
    "    v = fy * cam_points[:, 1] / z_safe + cy\n",
    "\n",
    "    coords = torch.stack([u, v], dim=-1)\n",
    "\n",
    "    return coords, z, cam_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630864f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_gaussian_points(points, colors, radii, pose, K, image_size, kernel_radius=3):\n",
    "    \"\"\"\n",
    "    Vectorized differentiable rendering of isotropic Gaussian splats.\n",
    "\n",
    "    Args:\n",
    "        points: (N, 3) tensor of world-space 3D points.\n",
    "        colors: (N, 3) RGB colors.\n",
    "        radii: (N,)  standard deviations (pixels) for each point in screen space.\n",
    "        pose: (4, 4) camera-to-world transform.\n",
    "        K: (3, 3) camera intrinsics.\n",
    "        image_size: int, output image size (square).\n",
    "        kernel_radius: int, splat radius in pixels.\n",
    "\n",
    "    Returns:\n",
    "        (3, H, W) image tensor.\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    H = W = image_size\n",
    "    N = points.shape[0]\n",
    "\n",
    "    # Project points to image plane\n",
    "    coords, _, _ = project_points(points, pose, K)  # (N, 2)\n",
    "    u, v = coords[:, 0], coords[:, 1]\n",
    "\n",
    "    # Build (2R+1)x(2R+1) pixel offset grid\n",
    "    k = 2 * kernel_radius + 1\n",
    "    offset_y, offset_x = torch.meshgrid(\n",
    "        torch.arange(-kernel_radius, kernel_radius + 1, device=device),\n",
    "        torch.arange(-kernel_radius, kernel_radius + 1, device=device),\n",
    "        indexing='ij'\n",
    "    )  # (k, k)\n",
    "\n",
    "    offsets = torch.stack([offset_x, offset_y], dim=-1).view(-1, 2)  # (K², 2)\n",
    "\n",
    "    # Expand all coordinates to per-pixel grid\n",
    "    coords_exp = coords.unsqueeze(1) + offsets.unsqueeze(0).float()  # (N, K², 2)\n",
    "    dx_dy = coords_exp - coords.unsqueeze(1)  # (N, K², 2)\n",
    "\n",
    "    # Compute squared distances and Gaussian weights\n",
    "    r2 = radii.clamp(min=1e-2).view(-1, 1) ** 2  # (N, 1)\n",
    "    distsq = dx_dy.pow(2).sum(dim=-1)  # (N, K²)\n",
    "    weights = torch.exp(-0.5 * distsq / r2)  # (N, K²)\n",
    "\n",
    "    # Map coords_exp to pixel indices\n",
    "    ix = coords_exp[..., 0].round().long().clamp(0, W - 1)\n",
    "    iy = coords_exp[..., 1].round().long().clamp(0, H - 1)\n",
    "    flat_idx = iy * W + ix  # (N, K²)\n",
    "\n",
    "    # Flatten for scatter\n",
    "    flat_idx = flat_idx.view(-1)            # (N*K²,)\n",
    "    flat_weights = weights.view(-1)         # (N*K²,)\n",
    "    expanded_colors = colors.unsqueeze(1).expand(-1, k*k, -1).reshape(-1, 3)  # (N*K², 3)\n",
    "\n",
    "    # Create flattened canvas\n",
    "    canvas = torch.zeros(3, H * W, device=device)\n",
    "    alpha = torch.zeros(H * W, device=device)\n",
    "\n",
    "    for c in range(3):\n",
    "        canvas[c].index_add_(0, flat_idx, flat_weights * expanded_colors[:, c])\n",
    "    alpha.index_add_(0, flat_idx, flat_weights)\n",
    "\n",
    "    # Reshape and normalize\n",
    "    canvas = canvas.view(3, H, W)\n",
    "    alpha = alpha.view(1, H, W).clamp(min=1e-5)\n",
    "    return (canvas / alpha).clamp(0.0, 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d20bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gaussians(points3D, device='cuda'):\n",
    "    \"\"\"\n",
    "    Initialize Gaussians exactly at points3D without noise.\n",
    "\n",
    "    Args:\n",
    "        points3D: tensor (N, 3) of 3D points\n",
    "        device: device string\n",
    "\n",
    "    Returns:\n",
    "        dict with keys 'xyz', 'color', 'radius' containing nn.Parameters\n",
    "    \"\"\"\n",
    "    xyz = points3D.to(device)\n",
    "\n",
    "    N = xyz.shape[0]\n",
    "    colors = torch.full((N, 3), 0.5, device=device) + 0.1 * torch.randn(N, 3, device=device)\n",
    "    colors = colors.clamp(0.0, 1.0)\n",
    "\n",
    "    radius = torch.full((N,), 1.0, device=device)\n",
    "\n",
    "    return {\n",
    "        'xyz': nn.Parameter(xyz),\n",
    "        'color': nn.Parameter(colors),\n",
    "        'radius': nn.Parameter(radius)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img(tensor_img, filename):\n",
    "    \"\"\"\n",
    "    Save a tensor image (C, H, W) to a file after clamping and cleaning invalid values.\n",
    "    \n",
    "    Args:\n",
    "        tensor_img: torch.Tensor of shape (C, H, W), with values expected in [0,1].\n",
    "        filename: str, path to save the image file.\n",
    "    \"\"\"\n",
    "    # Clamp values to valid range [0, 1]\n",
    "    safe_img = torch.clamp(tensor_img, 0.0, 1.0)\n",
    "\n",
    "    # Check for NaNs or infinite values and replace them safely\n",
    "    if torch.isnan(safe_img).any() or torch.isinf(safe_img).any():\n",
    "        print(f\"Warning: Image contains NaN or Inf values when saving {filename}\")\n",
    "        safe_img = torch.nan_to_num(safe_img, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "    # Convert tensor to HWC numpy uint8 image\n",
    "    img_np = (safe_img.permute(1, 2, 0).cpu().detach().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "    # Write image using imageio\n",
    "    iio.imwrite(filename, img_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(json_path, image_size=512, N_gaussians=250000, epochs=1000, lr=1e-2):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 1. Run SfM or load existing SfM results (poses_c2w and intrinsics as dicts keyed by filename)\n",
    "    poses_w2c_dict, intrinsics_dict, points3D = sfm_extract(\"nerf_synthetic/lego/train\", device=device)\n",
    "\n",
    "    # 2. Prepare fixed number of 3D points for gaussians\n",
    "    points3D = get_points(points3D, N_gaussians)\n",
    "\n",
    "    # 3. Initialize gaussians directly at SfM points (no noise)\n",
    "    gaussians = init_gaussians(points3D, device=device)\n",
    "\n",
    "    # 4. Create dataset with SfM poses and intrinsics dicts\n",
    "    dataset = NeRFDataset(\n",
    "        json_path,\n",
    "        image_size=image_size,\n",
    "        device=device,\n",
    "        sfm_poses=poses_w2c_dict,\n",
    "        sfm_intrinsics=intrinsics_dict\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(gaussians.values(), lr=lr)\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "    for epoch in pbar:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for image, pose_w2c, K in loader:\n",
    "            image = image.squeeze(0)\n",
    "            pose_w2c = pose_w2c.squeeze(0)\n",
    "            K = K.squeeze(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            render = render_gaussian_points(\n",
    "                gaussians['xyz'], gaussians['color'], gaussians['radius'],\n",
    "                pose_w2c, K, image_size\n",
    "            )\n",
    "\n",
    "            loss = F.mse_loss(render, image)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(loader)\n",
    "        pbar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            save_img(render, f\"render_{epoch+1}.png\")\n",
    "            save_img(image, f\"image_{epoch+1}.png\")\n",
    "            print(f\"Epoch {epoch+1} - Input image range: {image.min().item():.4f} to {image.max().item():.4f}\")\n",
    "            print(f\"Epoch {epoch+1} - Render image range: {render.min().item():.4f} to {render.max().item():.4f}\")\n",
    "\n",
    "    save_img(render, \"final.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dda8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\"nerf_synthetic/lego/transforms_train.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
